---
title: "Text mining"
author: "wu"
date: "2024-06-29"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Loading required package
# Install packages if needed
```{r}
install.packages(c("jiebaR", "tm", "wordcloud", "ggplot2", "dplyr", "readr", "showtext", "sysfonts", "openxlsx"))
```

```{r}
library(jiebaR)
library(tm)
library(wordcloud)
library(ggplot2)
library(dplyr)
library(readr)

```
# People's Daily S2
```{r}
PDS2 <- read_lines("People'sDailyS2.txt", locale = locale(encoding = "UTF-8"))

```

```{r}

corpus <- Corpus(VectorSource(PDS2))

```



##Clean texts
```{r}
corpus1 <- tm_map(corpus, removePunctuation)
corpus2 <- tm_map(corpus1, removeNumbers)
corpus3 <- tm_map(corpus2, stripWhitespace)
#Remove all special characters and punctuations
corpus4 <- tm_map(corpus3, content_transformer(function(x) {
  gsub("[！？｡＂＃＄％＆＇（）＊＋，－／：；＜＝＞＠［＼］＾＿｀｛｜｝～、。，·ˉˇ¨〃々—～‖…‘’“”〔〕〈〉《》「」『』【】︻︼﹍﹎￥％＋－＜＞＝～｜￤＆＊＠§※☆★○●◎◇◆□■△▲※〒→←↑↓〓︵︷︿︹︽︻︶︸﹀︺︾︼︵︶︷︸︹︺︻︼︽︾︿﹀﹁﹂﹃﹄︙︰︳︴︵︶︷︸︹︺︻︼︽︾︿﹀﹁﹂﹃﹄﹙﹚﹛﹜﹝﹞‘’“”〝〞〟＂｀´ˆˇˉ˘˙˚˛˜˝‘’“”‹›«»„‟〝〞〟＂]+", "", x, perl = TRUE)
}))

```
```{r}
#save processed texts
texts <- sapply(corpus4, as.character)
writeLines(texts, "text/processed_People'sDailyS2.txt")


```

# Chinese cutter
```{r}
segmenter <- worker()
seg_corpus <- tm_map(corpus4, content_transformer(function(x) {
  unlist(segment(x, segmenter))
}))
```

## WordFreq
```{r}
word_freqs <- table(seg_corpus$content)

df1 <- as.data.frame(word_freqs, stringsAsFactors = FALSE)
colnames(df1) <- c("word", "freq")

library(dplyr)
df1 <- df1 %>%
  arrange(desc(freq))

head(df1)
```


#Remove stopwords
```{r, include=FALSE}
library(stopwords)
en_stopwords <- stopwords(language = "en")
cn_stopwords <- read_lines("cn_stopwords.txt", locale = locale(encoding = "UTF-8"))

# Remove single-character words which has no meaning in Chinese
df2 <- df1 %>%
  filter(nchar(word) > 1)

df3 <- df2 %>%
  filter(!word %in% cn_stopwords) %>%  # Remove stopwords
  filter(!word %in% en_stopwords) %>% 
  filter(!word %in% c("AI", "音乐", "人工智能"))  # Remove specific high-frequency terms


# Due to the keyword matching method used in article retrieval, the occurrence frequency of the search theme terms "AI", "音乐" (music), and "人工智能" (artificial intelligence) is significantly higher than other terms. These terms are removed to focus on the frequency distribution of other vocabulary.
head(df3)

```


```{r, include=FALSE}
# 保存为 CSV 文件
write.csv(df3, "text/word_PDS2.csv", row.names = FALSE)
```
使用python将处理好的词频文件翻译为对应的英文，人工校对；因R中词云功能不支持不同语言混合的词云图生成，使用python生成词云。

#visualization
```{r}
#读取经过翻译处理的词频文件
# Read the translated word frequency file
df4 <- read_csv("text/translated_PDS2.csv", locale = locale(encoding = "UTF-8"))
df4 <- df4 %>%
  filter(!english_word %in% en_stopwords) 

df5 <- df4 %>% 
  arrange(desc(freq)) %>%  
  slice_head(n = 25)    

library(showtext)
font_add("PingFang", "/System/Library/Fonts/PingFang.ttc")  # Mac上的一种中文字体
showtext_auto()

  # 创建图表
p1 <- ggplot(df5, aes(x = reorder(word, freq), y = freq)) +
  geom_bar(stat = "identity", fill = "darkgreen") +
  coord_flip() +  # 将条形图翻转使单词在y轴上显示
  geom_text(aes(label = freq, hjust = -0.3), color = "black", size = 2) +
  labs(x = "Word", y = "Frequencies", title = "Word Frequency Distribution") +
  theme_minimal() +
  theme(axis.text.y = element_text(size = 8),  # 减小y轴字体大小
        axis.title.y = element_text(angle = 0, vjust = 0.5),
        text = element_text(family = "PingFang"))  # 调整y轴标题的位置和角度

p1e <- ggplot(df5, aes(x = reorder(english_word, freq), y = freq)) +
  geom_bar(stat = "identity", fill = "darkgreen") +
  coord_flip() +  # 将条形图翻转使单词在y轴上显示
  geom_text(aes(label = freq, hjust = -0.3), color = "black", size = 2) +
  labs(x = "Word", y = "Frequencies", title = "Word Frequency Distribution") +
  theme_minimal() +
  theme(axis.text.y = element_text(size = 8),  # 减小y轴字体大小
        axis.title.y = element_text(angle = 0, vjust = 0.5),
        text = element_text(family = "PingFang"))  # 调整y轴标题的位置和角度

# 输出图表
print(p1)
print(p1e)
ggsave("image/freqPDS2.png", plot = p1, width = 10, height = 8, dpi = 300)
ggsave("image/freqPDS2e.png", plot = p1e, width = 10, height = 8, dpi = 300)

```


##Sentiment Analysis

```{r}
library(openxlsx)
library(dplyr)
library(ggplot2)
library(tidyr)

# 读取情感词典
emotion_dict <- read.xlsx("dictionary/情感词汇本体.xlsx")
emotion_map <- setNames(emotion_dict$情感分类, emotion_dict$词语)

# 假设df3是你的词频表
# 步骤 2: 情感打标
df4$emotion <- emotion_map[df4$word]

# 提取情感强度和极性（假设这些列在情感词典中存在）
df4$intensity <- emotion_dict$强度[match(df4$word, emotion_dict$词语)]
df4$polarity <- emotion_dict$极性[match(df4$word, emotion_dict$词语)]

```


```{r}
# Adjust polarity and sentiment classification
df4 <- df4 %>%
  mutate(
    polarity_numeric = case_when(
      polarity == 0 ~ 0,  # Neutral: no impact
      polarity == 1 ~ 1,  # Positive
      polarity == 2 ~ -1, # Negative
      polarity == 3 ~ 0   # Both: may require different handling based on analysis needs
    ),
    sentiment = case_when(
      polarity == 0 ~ "Neutral",   
      polarity == 1 ~ "Positive",  
      polarity == 2 ~ "Negative",  
      polarity == 3 ~ "Both"       
    )
  )

# Count the total for each sentiment category
sentiment_counts <- df4 %>%
  filter(sentiment %in% c("Neutral", "Positive", "Negative", "Both")) %>%
  group_by(sentiment) %>%
  summarize(count = n(), .groups = "drop")

# Calculate percentages
total_count <- sum(sentiment_counts$count)  
sentiment_counts <- sentiment_counts %>%
  mutate(percentage = count / total_count * 100)

print(sentiment_counts)
```


```{r}
# 过滤含有 NA 的情感类别
df_filtered <- df4 %>%
  filter(!is.na(emotion)) %>%
  group_by(emotion) %>%
  summarize(total_freq = sum(freq))  # 汇总同一情感的频率

# 绘制情感分布的柱状图
p2 <- ggplot(df_filtered, aes(x = emotion, y = total_freq, fill = emotion)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = total_freq), vjust = -0.5, color = "black", size = 3) +  # 添加文本标签，vjust控制文本的垂直位置
  theme_minimal() +
  labs(x = "Emotion", y = "Frequency", title = "Stage2-People's Daily-Sentiment") +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_y_continuous(breaks = seq(0, max(df_filtered$total_freq), by = 50))

ggsave("image/emotionPDS2.png", plot = p2, width = 10, height = 8, dpi = 300)
print(p2)
```

```{r}

# Filter data with non-NA sentiments and adjust the ordering and slicing of the data
df6 <- df4 %>%
  filter(!is.na(sentiment)) %>%
  arrange(desc(freq)) %>%  
  slice_head(n = 25) %>% 
  mutate(sentiment = factor(sentiment, levels = c("Neutral", "Positive", "Negative", "Both")))

# Create the chart with proper labels and a color scheme that includes all categories
p3 <- ggplot(df6, aes(x = reorder(word, freq), y = freq, fill = sentiment)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = freq, hjust = -0.3), color = "black", size = 2) +
  coord_flip() +
  scale_fill_manual(values = c("Neutral" = "grey", "Positive" = "turquoise", "Negative" = "orange", "Both" = "purple")) +
  labs(x = "Word", y = "Frequency", title = "Sentiment Distribution of Top Words") +
  theme_minimal()

p3e <- ggplot(df6, aes(x = reorder(english_word, freq), y = freq, fill = sentiment)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = freq, hjust = -0.3), color = "black", size = 2) +
  coord_flip() +
  scale_fill_manual(values = c("Neutral" = "grey", "Positive" = "turquoise", "Negative" = "orange", "Both" = "purple")) +
  labs(x = "Word", y = "Frequency", title = "Sentiment Distribution of Top Words") +
    theme_minimal()

# 显示图表
print(p3)
print(p3e)

ggsave("image/sentimentPDS2.png", plot = p3, width = 10, height = 8, dpi = 300)
ggsave("image/sentimentPDS2e.png", plot = p3e, width = 10, height = 8, dpi = 300)
```


```{r}
# Filter top data for negative sentiment
df_negative <- df4 %>%
  filter(sentiment == "Negative")%>%
  arrange(desc(freq)) %>%  
  slice_head(n = 15)

# Create the plot for negative sentiment words
p_negative <- ggplot(df_negative, aes(x = reorder(word, freq), y = freq, fill = sentiment)) +
  geom_bar(stat = "identity") +  # Use bars to represent frequency
  geom_text(aes(label = freq, hjust = -0.3), color = "black", size = 2) +
  coord_flip() +  # Flip coordinates for better readability of words
  scale_y_continuous(breaks = seq(0, max(df_negative$freq), by = 50)) +  
  scale_fill_manual(values = c("Negative" = "orange")) +  # Set color for negative sentiment
  labs(x = "Word", y = "Frequency", title = "Frequency Distribution of Negative Sentiment Words") +
  theme_minimal()  # Use a minimalistic theme

# Display the plot for negative sentiment words
print(p_negative)

# Create the plot for negative sentiment words in english
pe_negative <- ggplot(df_negative, aes(x = reorder(english_word, freq), y = freq, fill = sentiment)) +
  geom_bar(stat = "identity") +  # Use bars to represent frequency
  geom_text(aes(label = freq, hjust = -0.3), color = "black", size = 2) +
  coord_flip() +  # Flip coordinates for better readability of words
  scale_y_continuous(breaks = seq(0, max(df_negative$freq), by = 50)) +  # Set y-axis breaks every 1 unit
  scale_fill_manual(values = c("Negative" = "orange")) +  # Set color for negative sentiment
  labs(x = "Word", y = "Frequency", title = "Frequency Distribution of Negative Sentiment Words") +
  theme_minimal()  # Use a minimalistic theme

# Display the plot for negative sentiment words
print(pe_negative)

# Save the plot to a file
ggsave("image/negative_sentiment_PDS2.png", plot = p_negative, width = 10, height = 8, dpi = 300)
ggsave("image/negative_sentiment_PDS2e.png", plot = p_negative, width = 10, height = 8, dpi = 300)

```













